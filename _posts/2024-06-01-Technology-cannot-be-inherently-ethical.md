---
layout: post
---

![alt text](/assets/pinkrobotincornmaze.jpg)
Image made with DALL-E

**Introduction**

While technology is a system of actions to change or reform nature and those actions can be “good”, I am arguing for the position that technology cannot be inherently ethical. Technology is neither inherently “good” nor “bad”, because value systems and technological systems are autonomous until integrated, the context of creation and use of technology determines its effect, and most  technology is not intelligent. 

**Technological systems and value systems are autonomous until integrated**

In a review of a report released by the Soil Association titled, “AgroEcoTech: How Can Technology Accelerate a Transition to Agroecology?”, Lawrence Woodward and Pat Thomas [^1] criticize the report’s exclusion of values such as food sovereignty, social justice, equity, care and fairness that are fundamental to the practice of agroecology and the related international principled organic farming movement. The authors of AgroEcoTech report, Cumulus Consultants, chose to show few, if any connections between technological systems and value-systems of the agroecology movement. 

Woodward and Thomas argue that the AgroEcoTech report presents technology as “values neutral”. Instead I propose that the technological systems in the report are either autonomous, have weak connectivity with the value-systems of the agroecology movement, or are integrated with value-systems that conflict with the value-systems of the agroecology movement. 

A system is a group of connected components that work together as a whole. [^2] The ability of a system to function independently from its surroundings or other systems defines its autonomy. Technological systems are autonomous, and cannot be measured as good or bad until integrated into one of many value systems. When two systems integrate, changes in one system are likely to cause changes in another, additionally, properties may emerge from each system as a result of integration. For example, when a value system and technological system integrate, a new way of thinking or behaving may emerge as a result.  

**The context of the creation and use of technology determine the effect of technology**

To understand how one may measure the ethicality of a technology by the context of its creation and use an example may be helpful. I use “solar panels” to exemplify this premise. 

Solar panels convert sunlight into electricity, providing a renewable source of energy alternative to oil, natural gas, and coal. The solar panel, a symbol of sustainability, is not an inherently ethical technology. Solar panel batteries require the extraction of lithium. Environmental-degradation is in the context of the creation of solar panels, which becomes a factor  determining the overall effect of the technology. Additionally, geographies -a context of creation- vary in their magnitude of solar intensity.[^3] This means the opportunity for people to benefit from solar panels is naturally unequal.

The contexts in which solar panels may be used are diverse. A solar panel may power a school in a less-developed country or a robot roaming Mars. Solar panels, a technology, offer literal and metaphorical power. According to the textbook, Technology in Action, the first and least difficult concept for readers are introduced to, is that “technology offers them power to change their society and world.” [^4] In agreement, Sharon Hare, author of the book, Technology is Not Neutral , writes that technology “... is about power. How we approach that power is shaped by our values.” [^5] The “shaping” of power Hare describes may be akin to the way values can contextualize the use of technology. Most readers of this blogpost are end-users with little control over the supply chain behind solar panels. Our values contextualize use of solar panels, which will result in a “good” or “bad” effect.

**Most technology is not “intelligent”**

In the study, “Ethical aspects of AI robots for agri‑food; a relational approach based on four case studies”, 33 experts in the development and implementation of AI robots for the agri-food industry were interviewed. [^6] The researchers write, “AI robots adapt their actions to the environment, they are able to learn, solve problems, anticipate consequences of certain courses of action and reason about which one to choose.” The degree of “intelligence” or “autonomy” developers give AI robots brings into question how much responsibility is also given. Giving an AI robot full autonomy risks the possibility that the AI robot would “hurt someone, or an animal, or [break] a barn or a greenhouse”. 

Today, “intelligence” of AI is being debated. In the conclusion of the study shared, the debate is discouraged and moves the conversation to be about better robot-human relations.  The researchers found literature discussing the ethics of AI robots focused on the issue of AI robot “intelligence”, while those interviewed in the study rarely compared the “intelligence” of robots to that of humans. Humans, certainly, have “intelligence” and can be held responsible for our actions. The day where technology itself decides to make “good” or “bad” choices and can be held responsible for the consequences is not here yet.
We can chose to create “ethical technology”

Some may envision a world where humans develop and maintain technology that, by design, influences user behavior for the “good” of society. The belief that technology cannot be inherently ethical may limit our potential. In “Chapter 2: The Ethics of Engineering” of Nick Dotyʻs dissertation, the author builds a perspective of technology as synonymous with well-being using the argument of José Ortega y Gasset. [^7] Foundational to Ortegaʻs argument are his definitions of technology as an act or system of actions. Ortega writes, “technology is the distinctly human act of changing or reforming nature.”, and later in a different context writes, “- technology is a system of actions called forth and directed by [necessities].” 

If technology is a system of actions to change or reform nature, each action is also an opportunity to do “good” or “bad”. Each action is also a choice. The question, “Can humans bring forth ethical technology?”, becomes, “Can humans make ethical choices?” By this logic, technology _can_ be ethical, however, in a article titled “The Brief History of Decision Making” Leigh Buchanan and Andrew O’Connell [^8] warn that, “a good decision does not guarantee a good outcome”.This means although technology is a system of actions to change or reform nature and those actions can be “good” if preceded by “good” choices, this process still may not result in “good” or “ethical technology”.

**In summary**

According to the framework of thinking established in this blogpost, “ethical” applies to values, actions, choices, processes, and use, but not technology. When the adverb, “ethical” modifies the word “technology” it presents “ethical” as a quality or inherent property of “technology”. Technology is not inherently ethical because it is a system autonomous from individual or organizational value systems until integrated. The effect of technology can be judged as ethical or not, by its context of creation and use. And since unlike humans, most technology is not “intelligent”, technology cannot, on its own, make ethical actions. 

**Footnotes**

[^1]: Woodward, Lawrence, and Pat Thomas. “The Agroecotech Trap.” A Bigger Conversation, 14 Jan. 2022, https://www.abiggerconversation.org/the-agroecotech-trap/
[^2]: Systems Innovation Network. “Integrated Systems.” YouTube, YouTube, 25 Sept. 2016, https://www.youtube.com/watch?v=dD_D2X5ljgQ&list=PLOB32hTn5Lt-aA9Q4HccYZo03B11EmSHv&index=26. 
[^3]: Zeihan, Peter. “The Solar Power Problem(s).” Zeihan on Geopolitics, 26 Sept. 2023, https://www.zeihan.com/the-solar-power-problems/. 
[^4]: Evans, Alan, et al. Technology in Action. 17th ed., Pearson, 2022.
[^5]: Hare, Stephanie. Technology Is Not Neutral A Short Guide to Technology Ethics. London Publishing Partnership, 2022. 
[^6]: Van der Burg, Simone, et al. “Ethical aspects of AI Robots for Agri-Food; a relational approach based on four case studies.” AI &amp; SOCIETY, vol. 39, no. 2, 1 Apr. 2022, pp. 541–555, https://doi.org/10.1007/s00146-022-01429-8. 
[^7]: Nick Doty. Enacting Privacy in Internet Standards. Ph.D. dissertation. Advisor: Deirdre K. Mulligan. University of California, Berkeley. 2020. https://npdoty.name/enacting-privacy/.
[^8]: Buchanan, Leigh, and Andrew O’Connell. “A Brief History of Decision Making.” Harvard Business Review, 31 Aug. 2021, https://www.hbr.org/2006/01/a-brief-history-of-decision-making. 

